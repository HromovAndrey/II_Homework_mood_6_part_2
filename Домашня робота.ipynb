{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/andrey36912/notebook2008c085c5?scriptVersionId=194443497\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-28T14:56:22.349532Z","iopub.execute_input":"2024-08-28T14:56:22.350326Z","iopub.status.idle":"2024-08-28T14:56:22.361204Z","shell.execute_reply.started":"2024-08-28T14:56:22.350261Z","shell.execute_reply":"2024-08-28T14:56:22.359566Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"!pip install optuna","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:56:22.364204Z","iopub.execute_input":"2024-08-28T14:56:22.365874Z","iopub.status.idle":"2024-08-28T14:56:38.794151Z","shell.execute_reply.started":"2024-08-28T14:56:22.365798Z","shell.execute_reply":"2024-08-28T14:56:38.792772Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.2)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.30)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.4)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report\nimport nltk\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:56:38.796067Z","iopub.execute_input":"2024-08-28T14:56:38.796524Z","iopub.status.idle":"2024-08-28T14:56:38.805432Z","shell.execute_reply.started":"2024-08-28T14:56:38.796476Z","shell.execute_reply":"2024-08-28T14:56:38.804086Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf1 = pd.read_csv(\"https://raw.githubusercontent.com/HalyshAnton/IT-Step-Pyton-AI/main/module6/data/business_data.csv\")\ndf2 = pd.read_csv(\"https://raw.githubusercontent.com/HalyshAnton/IT-Step-Pyton-AI/main/module6/data/education_data.csv\")\ndf3 = pd.read_csv(\"https://raw.githubusercontent.com/HalyshAnton/IT-Step-Pyton-AI/main/module6/data/entertainment_data.csv\")\ndf4 = pd.read_csv(\"https://raw.githubusercontent.com/HalyshAnton/IT-Step-Pyton-AI/main/module6/data/sports_data.csv\")\ndf5 = pd.read_csv(\"https://raw.githubusercontent.com/HalyshAnton/IT-Step-Pyton-AI/main/module6/data/technology_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:56:38.808689Z","iopub.execute_input":"2024-08-28T14:56:38.809144Z","iopub.status.idle":"2024-08-28T14:56:40.315912Z","shell.execute_reply.started":"2024-08-28T14:56:38.8091Z","shell.execute_reply":"2024-08-28T14:56:40.314319Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"df1['labels'] = 'business'\ndf2['labels'] = 'education'\ndf3['labels'] = 'entertainment'\ndf4['labels'] = 'sports'\ndf5['labels'] = 'technology'","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:56:40.317821Z","iopub.execute_input":"2024-08-28T14:56:40.318386Z","iopub.status.idle":"2024-08-28T14:56:40.328394Z","shell.execute_reply.started":"2024-08-28T14:56:40.318338Z","shell.execute_reply":"2024-08-28T14:56:40.327089Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n\ndf['text'] = df['headlines'].fillna('') + ' ' + df['description'].fillna('') + ' ' + df['content'].fillna('')\n\ndf.drop(['headlines', 'description', 'content', 'url'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:56:40.330008Z","iopub.execute_input":"2024-08-28T14:56:40.330751Z","iopub.status.idle":"2024-08-28T14:56:40.381077Z","shell.execute_reply.started":"2024-08-28T14:56:40.330704Z","shell.execute_reply":"2024-08-28T14:56:40.379746Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['text'], df['labels'], test_size=0.2, random_state=42)\n\nvectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:56:40.382742Z","iopub.execute_input":"2024-08-28T14:56:40.383164Z","iopub.status.idle":"2024-08-28T14:56:52.121173Z","shell.execute_reply.started":"2024-08-28T14:56:40.383121Z","shell.execute_reply":"2024-08-28T14:56:52.119366Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"dt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(X_train_tfidf, y_train)\ny_pred_dt = dt_model.predict(X_test_tfidf)\nprint(\"DecisionTreeClassifier Report:\")\nprint(classification_report(y_test, y_pred_dt))\n\nnb_model = GaussianNB()\nnb_model.fit(X_train_tfidf.toarray(), y_train)\ny_pred_nb = nb_model.predict(X_test_tfidf.toarray())\nprint(\"GaussianNB Report:\")\nprint(classification_report(y_test, y_pred_nb))\n\nsvc_linear_model = SVC(kernel=\"linear\", random_state=42)\nsvc_linear_model.fit(X_train_tfidf, y_train)\ny_pred_svc_linear = svc_linear_model.predict(X_test_tfidf)\nprint(\"SVC with linear kernel Report:\")\nprint(classification_report(y_test, y_pred_svc_linear))\n\nsvc_rbf_model = SVC(kernel=\"rbf\", random_state=42)\nsvc_rbf_model.fit(X_train_tfidf, y_train)\ny_pred_svc_rbf = svc_rbf_model.predict(X_test_tfidf)\nprint(\"SVC with rbf kernel Report:\")\nprint(classification_report(y_test, y_pred_svc_rbf))\n\nmlp_model = MLPClassifier(random_state=42)\nmlp_model.fit(X_train_tfidf, y_train)\ny_pred_mlp = mlp_model.predict(X_test_tfidf)\nprint(\"MLPClassifier Report:\")\nprint(classification_report(y_test, y_pred_mlp))","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:56:52.123548Z","iopub.execute_input":"2024-08-28T14:56:52.124331Z","iopub.status.idle":"2024-08-28T14:59:23.76961Z","shell.execute_reply.started":"2024-08-28T14:56:52.124257Z","shell.execute_reply":"2024-08-28T14:59:23.768397Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"DecisionTreeClassifier Report:\n               precision    recall  f1-score   support\n\n     business       0.86      0.87      0.87       410\n    education       0.91      0.90      0.91       393\nentertainment       0.94      0.94      0.94       395\n       sports       0.90      0.90      0.90       415\n   technology       0.83      0.83      0.83       387\n\n     accuracy                           0.89      2000\n    macro avg       0.89      0.89      0.89      2000\n weighted avg       0.89      0.89      0.89      2000\n\nGaussianNB Report:\n               precision    recall  f1-score   support\n\n     business       0.94      0.96      0.95       410\n    education       0.98      0.96      0.97       393\nentertainment       0.97      0.97      0.97       395\n       sports       0.97      0.98      0.97       415\n   technology       0.94      0.93      0.93       387\n\n     accuracy                           0.96      2000\n    macro avg       0.96      0.96      0.96      2000\n weighted avg       0.96      0.96      0.96      2000\n\nSVC with linear kernel Report:\n               precision    recall  f1-score   support\n\n     business       0.98      0.97      0.98       410\n    education       1.00      0.99      0.99       393\nentertainment       1.00      1.00      1.00       395\n       sports       1.00      0.99      0.99       415\n   technology       0.96      0.97      0.97       387\n\n     accuracy                           0.99      2000\n    macro avg       0.99      0.99      0.99      2000\n weighted avg       0.99      0.99      0.99      2000\n\nSVC with rbf kernel Report:\n               precision    recall  f1-score   support\n\n     business       0.98      0.97      0.98       410\n    education       1.00      0.99      0.99       393\nentertainment       1.00      1.00      1.00       395\n       sports       1.00      1.00      1.00       415\n   technology       0.96      0.97      0.97       387\n\n     accuracy                           0.99      2000\n    macro avg       0.99      0.99      0.99      2000\n weighted avg       0.99      0.99      0.99      2000\n\nMLPClassifier Report:\n               precision    recall  f1-score   support\n\n     business       0.98      0.98      0.98       410\n    education       0.99      0.99      0.99       393\nentertainment       0.99      1.00      1.00       395\n       sports       1.00      1.00      1.00       415\n   technology       0.97      0.97      0.97       387\n\n     accuracy                           0.99      2000\n    macro avg       0.99      0.99      0.99      2000\n weighted avg       0.99      0.99      0.99      2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:59:23.771048Z","iopub.execute_input":"2024-08-28T14:59:23.771419Z","iopub.status.idle":"2024-08-28T14:59:23.776596Z","shell.execute_reply.started":"2024-08-28T14:59:23.771379Z","shell.execute_reply":"2024-08-28T14:59:23.775459Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"nltk.data.path.append('/usr/share/nltk_data')","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:59:23.78067Z","iopub.execute_input":"2024-08-28T14:59:23.781034Z","iopub.status.idle":"2024-08-28T14:59:23.791353Z","shell.execute_reply.started":"2024-08-28T14:59:23.780996Z","shell.execute_reply":"2024-08-28T14:59:23.789748Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"nltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:59:23.792869Z","iopub.execute_input":"2024-08-28T14:59:23.793276Z","iopub.status.idle":"2024-08-28T14:59:23.808639Z","shell.execute_reply.started":"2024-08-28T14:59:23.793212Z","shell.execute_reply":"2024-08-28T14:59:23.807578Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\nprint(lemmatizer.lemmatize(\"running\"))","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:59:23.81021Z","iopub.execute_input":"2024-08-28T14:59:23.810594Z","iopub.status.idle":"2024-08-28T14:59:23.979208Z","shell.execute_reply.started":"2024-08-28T14:59:23.810555Z","shell.execute_reply":"2024-08-28T14:59:23.977531Z"},"trusted":true},"execution_count":55,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/share/nltk_data'\n**********************************************************************","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/stem/wordnet.py:40\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39mNOUN):\n\u001b[0;32m---> 40\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/share/nltk_data'\n**********************************************************************"],"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/share/nltk_data'\n**********************************************************************","output_type":"error"}]},{"cell_type":"code","source":"stemmer = PorterStemmer()\nstop_words = set(stopwords.words('english'))\n\nclass MyTokenizer:\n    def __init__(self, use_lemmatization=True, use_stemming=False):\n        self.use_lemmatization = use_lemmatization\n        self.use_stemming = use_stemming\n\n    def tokenize(self, text):\n        words = nltk.word_tokenize(text)\n        if self.use_lemmatization:\n            words = [lemmatizer.lemmatize(word) for word in words]\n        if self.use_stemming:\n            words = [stemmer.stem(word) for word in words]\n        words = [word for word in words if word.isalnum() and word not in stop_words]\n        return ' '.join(words)\n\n# Применение токенизатора к тексту\ntokenizer = MyTokenizer(use_lemmatization=True, use_stemming=True)\ndf['text'] = df['text'].apply(tokenizer.tokenize)","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:59:23.980139Z","iopub.status.idle":"2024-08-28T14:59:23.980582Z","shell.execute_reply.started":"2024-08-28T14:59:23.980367Z","shell.execute_reply":"2024-08-28T14:59:23.980389Z"},"trusted":true},"execution_count":null,"outputs":[]}]}